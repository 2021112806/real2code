"""
ä½¿ç”¨å¾®è°ƒåçš„CodeLlamaæ¨¡å‹å¯¹sample_bbox.jsonè¿›è¡Œæ¨ç†

ç”¨æ³•:
# æµ‹è¯•æ‰€æœ‰æ ·æœ¬
python inference_example.py --lora_model code-llama-shape-ft/checkpoint-400

# æµ‹è¯•å•ä¸ªæ ·æœ¬
python inference_example.py --lora_model code-llama-shape-ft/checkpoint-400 --sample_name StorageFurniture_simple

# ä½¿ç”¨åŸºç¡€æ¨¡å‹ï¼ˆæœªå¾®è°ƒï¼‰
python inference_example.py
"""

import torch
from transformers import AutoModelForCausalLM, LlamaTokenizer
from peft import PeftModel
import json
import argparse
import os


def load_model(base_model_path, lora_model_path=None):
    """
    åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
    
    å‚æ•°:
        base_model_path: åŸºç¡€æ¨¡å‹è·¯å¾„
        lora_model_path: LoRAæƒé‡è·¯å¾„ï¼ˆå¯é€‰ï¼‰
    
    è¿”å›:
        model: åŠ è½½çš„æ¨¡å‹
        tokenizer: åˆ†è¯å™¨
    """
    print(f"åŠ è½½åŸºç¡€æ¨¡å‹: {base_model_path}")
    
    # åŠ è½½åŸºç¡€æ¨¡å‹
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_path,
        torch_dtype=torch.float16,
        device_map="auto",
        low_cpu_mem_usage=True,
    )
    
    # å¦‚æœæä¾›äº†LoRAæƒé‡ï¼ŒåŠ è½½å®ƒ
    if lora_model_path and os.path.exists(lora_model_path):
        print(f"åŠ è½½LoRAæƒé‡: {lora_model_path}")
        model = PeftModel.from_pretrained(base_model, lora_model_path)
        model = model.merge_and_unload()  # åˆå¹¶LoRAæƒé‡åˆ°åŸºç¡€æ¨¡å‹
        print("âœ“ ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹")
    else:
        model = base_model
        if lora_model_path:
            print(f"âš ï¸  LoRAæƒé‡ä¸å­˜åœ¨: {lora_model_path}")
        print("âœ“ ä½¿ç”¨åŸºç¡€æ¨¡å‹ï¼ˆæœªå¾®è°ƒï¼‰")
    
    # åŠ è½½åˆ†è¯å™¨ (ä½¿ç”¨LlamaTokenizerä»£æ›¿AutoTokenizer)
    tokenizer = LlamaTokenizer.from_pretrained(base_model_path)
    tokenizer.pad_token_id = 0
    tokenizer.padding_side = "left"
    
    print(f"æ¨¡å‹åŠ è½½å®Œæˆ")
    print(f"Tokenizer eos_token_id: {tokenizer.eos_token_id}")
    print(f"Model eos_token_id: {model.config.eos_token_id}")
    print()
    return model, tokenizer


def compare_outputs(generated, expected):
    """æ¯”è¾ƒç”Ÿæˆçš„è¾“å‡ºå’ŒæœŸæœ›çš„è¾“å‡º"""
    print("\n" + "="*80)
    print("ğŸ“Š ç»“æœæ¯”è¾ƒ")
    print("="*80)
    
    print("\næœŸæœ›è¾“å‡º:")
    print("-"*80)
    print(expected)
    
    print("\nå®é™…ç”Ÿæˆ:")
    print("-"*80)
    print(generated)
    
    print("\n" + "="*80)
    if generated.strip() == expected.strip():
        print("âœ… å®Œå…¨åŒ¹é…ï¼")
    else:
        match_score = 0
        total_checks = 2
        
        # æ£€æŸ¥root_geom
        if 'root_geom' in generated:
            gen_root = generated.split('root_geom')[1].split('\n')[0].strip()
            exp_root = expected.split('root_geom')[1].split('\n')[0].strip()
            if gen_root == exp_root:
                print("âœ… root_geom åŒ¹é…")
                match_score += 1
            else:
                print(f"âŒ root_geom ä¸åŒ¹é…: æœŸæœ› '{exp_root}', å®é™… '{gen_root}'")
        
        # æ£€æŸ¥child_jointsæ•°é‡
        gen_dict_count = generated.count('dict(')
        exp_dict_count = expected.count('dict(')
        if gen_dict_count == exp_dict_count:
            print(f"âœ… å…³èŠ‚æ•°é‡åŒ¹é…: {gen_dict_count}")
            match_score += 1
        else:
            print(f"âŒ å…³èŠ‚æ•°é‡ä¸åŒ¹é…: æœŸæœ› {exp_dict_count}, å®é™… {gen_dict_count}")
        
        accuracy = (match_score / total_checks) * 100
        print(f"\nåŒ¹é…åº¦: {accuracy:.1f}% ({match_score}/{total_checks})")
    print("="*80)


def create_prompt(bbox_code):
    """
    åˆ›å»ºæ¨ç†prompt
    
    å‚æ•°:
        bbox_code: OBBä»£ç å­—ç¬¦ä¸²
    
    è¿”å›:
        prompt: å®Œæ•´çš„prompt
    """
    prompt = f"""You are an AI assistant trained to understand 3D scenes and object relationships. Given the following Oriented Bounding Box (OBB) information, your task is to generate a list of child joints that describes the articulations between object parts.

OBB Information:
### Input:
{bbox_code}

Generate a number of root_geom,which means the base object,relative to OBB ID
- root_geom: Integer relative to/ selected from  input OBB ID
Generate a list of child joints. Each joint should be described by a dictionary with the following keys:
- box: The ID of the child bounding box
- type: The joint type ('hinge' for revolute joints, 'slide' for prismatic joints)
- idx: The rotation axis index (0 for x-axis, 1 for y-axis, 2 for z-axis)
- edge: Edge coordinates on the OBB, for example [1, -1]
- sign: Direction of the joint (+1 or -1)

IMPORTANT: Your response must contain ONLY the root_geom number and child_joints list, exactly as shown below, with no additional text before or after:

root_geom=[root_geom_number] 
child_joints = [
    dict(box=[child OBB ID], type=[joint type], idx=[rotation axis index], edge=[edge coordinates], sign=[direction]),
    # Additional joints as needed
]


Generate the geom_number and child_joints list:

### Response:
"""
    return prompt


def generate_joint_code(model, tokenizer, bbox_code, max_new_tokens=256):
    """
    ç”Ÿæˆå…³èŠ‚ä»£ç 
    
    å‚æ•°:
        model: æ¨¡å‹
        tokenizer: åˆ†è¯å™¨
        bbox_code: OBBä»£ç 
        max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
    
    è¿”å›:
        generated_code: ç”Ÿæˆçš„å…³èŠ‚ä»£ç 
    """
    # åˆ›å»ºprompt
    prompt = create_prompt(bbox_code)
    
    # ç¼–ç è¾“å…¥
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
    # ç”Ÿæˆï¼ˆä½¿ç”¨è´ªå©ªè§£ç é¿å…é‡‡æ ·é—®é¢˜ï¼‰
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=False,  # è´ªå©ªè§£ç ï¼Œæ›´ç¨³å®š
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )
    
    # è§£ç è¾“å‡º
    full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # è°ƒè¯•ï¼šæ‰“å°ç”Ÿæˆçš„tokenæ•°é‡å’Œéƒ¨åˆ†å†…å®¹
    input_length = inputs['input_ids'].shape[1]
    output_length = outputs.shape[1]
    generated_length = output_length - input_length
    print(f"ç”Ÿæˆäº† {generated_length} ä¸ªæ–°token")
    
    # è°ƒè¯•ï¼šæ‰“å°å®Œæ•´è¾“å‡ºçš„æœ€å200ä¸ªå­—ç¬¦
    print(f"å®Œæ•´è¾“å‡ºæœ«å°¾: ...{full_output[-200:]}")
    
    # æå–ç”Ÿæˆçš„éƒ¨åˆ†ï¼ˆåœ¨"### Response:"ä¹‹åï¼‰
    if "### Response:" in full_output:
        generated_code = full_output.split("### Response:")[-1].strip()
        print(f"æå–çš„ç”Ÿæˆå†…å®¹é•¿åº¦: {len(generated_code)}")
    else:
        print("âš ï¸  è­¦å‘Š: è¾“å‡ºä¸­æ²¡æœ‰æ‰¾åˆ°'### Response:'æ ‡è®°")
        generated_code = full_output
    
    # å¦‚æœç”Ÿæˆä¸ºç©ºï¼Œè¿”å›å®Œæ•´è¾“å‡ºç”¨äºè°ƒè¯•
    if not generated_code:
        print("âš ï¸  è­¦å‘Š: ç”Ÿæˆå†…å®¹ä¸ºç©º")
    
    return generated_code


def main():
    parser = argparse.ArgumentParser(description="ä½¿ç”¨å¾®è°ƒåçš„CodeLlamaè¿›è¡Œæ¨ç†")
    parser.add_argument(
        "--base_model",
        type=str,
        default="/mnt/data/zhangzhaodong/real2code/models/codellama-7b",
        help="åŸºç¡€CodeLlamaæ¨¡å‹è·¯å¾„",
    )
    parser.add_argument(
        "--lora_model",
        type=str,
        default=None,
        help="LoRAæƒé‡è·¯å¾„ï¼ˆå¯é€‰ï¼‰",
    )
    parser.add_argument(
        "--sample_file",
        type=str,
        default="sample_bbox.json",
        help="æ ·æœ¬æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤: sample_bbox.jsonï¼‰",
    )
    parser.add_argument(
        "--sample_name",
        type=str,
        default=None,
        help="è¦æµ‹è¯•çš„æ ·æœ¬åç§°ï¼ˆå¦‚æœä¸æŒ‡å®šï¼Œå°†æµ‹è¯•æ‰€æœ‰æ ·æœ¬ï¼‰",
    )
    parser.add_argument(
        "--max_new_tokens",
        type=int,
        default=256,
        help="æœ€å¤§ç”Ÿæˆtokenæ•°",
    )
    parser.add_argument(
        "--output_file",
        type=str,
        default="inference_results.txt",
        help="ç»“æœä¿å­˜æ–‡ä»¶",
    )
    
    args = parser.parse_args()
    
    print("=" * 80)
    print("ğŸ¤– CodeLlama å…³èŠ‚ä»£ç ç”Ÿæˆæ¨ç†")
    print("=" * 80)
    print()
    
    # åŠ è½½æ¨¡å‹
    model, tokenizer = load_model(args.base_model, args.lora_model)
    model.eval()  # ç¡®ä¿æ¨¡å‹å¤„äºè¯„ä¼°æ¨¡å¼
    
    # è¯»å–æ ·æœ¬æ•°æ®
    if not os.path.exists(args.sample_file):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ ·æœ¬æ–‡ä»¶ {args.sample_file}")
        return
    
    with open(args.sample_file, "r") as f:
        samples = json.load(f)
    
    # ç¡®å®šè¦æµ‹è¯•çš„æ ·æœ¬
    if args.sample_name:
        if args.sample_name not in samples:
            print(f"âŒ é”™è¯¯: æ ·æœ¬ '{args.sample_name}' ä¸å­˜åœ¨")
            print(f"å¯ç”¨æ ·æœ¬: {list(samples.keys())}")
            return
        test_samples = {args.sample_name: samples[args.sample_name]}
    else:
        test_samples = samples
    
    print(f"ğŸ“‹ å°†æµ‹è¯• {len(test_samples)} ä¸ªæ ·æœ¬\n")
    
    # ä¿å­˜æ‰€æœ‰ç»“æœ
    all_results = []
    
    # å¯¹æ¯ä¸ªæ ·æœ¬è¿›è¡Œæ¨ç†
    for sample_name, sample_data in test_samples.items():
        print("=" * 80)
        print(f"ğŸ” æµ‹è¯•æ ·æœ¬: {sample_name}")
        print("=" * 80)
        
        if "description" in sample_data:
            print(f"è¯´æ˜: {sample_data['description']}")
        
        bbox_code = sample_data["obb_rel"]["bbox_code"]
        expected_output = sample_data["obb_rel"].get("expected_output", "")
        
        print("\nğŸ“¥ è¾“å…¥OBBä»£ç :")
        print("-" * 80)
        print(bbox_code)
        print("-" * 80)
        
        # ç”Ÿæˆå…³èŠ‚ä»£ç 
        print("\nâš™ï¸  æ­£åœ¨ç”Ÿæˆ...")
        generated_code = generate_joint_code(
            model, tokenizer, bbox_code,
            max_new_tokens=args.max_new_tokens
        )
        
        # å¦‚æœæœ‰æœŸæœ›è¾“å‡ºï¼Œè¿›è¡Œæ¯”è¾ƒ
        if expected_output:
            compare_outputs(generated_code, expected_output)
        else:
            print("\nğŸ“¤ ç”Ÿæˆçš„å…³èŠ‚ä»£ç :")
            print("=" * 80)
            print(generated_code)
            print("=" * 80)
        
        # ä¿å­˜ç»“æœ
        all_results.append({
            "sample_name": sample_name,
            "bbox_code": bbox_code,
            "expected_output": expected_output,
            "generated_output": generated_code
        })
        
        print()
    
    # ä¿å­˜æ‰€æœ‰ç»“æœåˆ°æ–‡ä»¶
    with open(args.output_file, "w", encoding="utf-8") as f:
        f.write("CodeLlama å…³èŠ‚ä»£ç ç”Ÿæˆæ¨ç†ç»“æœ\n")
        f.write("=" * 80 + "\n\n")
        f.write(f"æ¨¡å‹: {args.base_model}\n")
        if args.lora_model:
            f.write(f"LoRA: {args.lora_model}\n")
        f.write(f"æµ‹è¯•æ ·æœ¬æ•°: {len(test_samples)}\n")
        f.write("\n" + "=" * 80 + "\n\n")
        
        for result in all_results:
            f.write(f"æ ·æœ¬: {result['sample_name']}\n")
            f.write("-" * 80 + "\n")
            f.write(f"\nè¾“å…¥OBBä»£ç :\n{result['bbox_code']}\n")
            f.write(f"\næœŸæœ›è¾“å‡º:\n{result['expected_output']}\n")
            f.write(f"\nç”Ÿæˆè¾“å‡º:\n{result['generated_output']}\n")
            f.write("\n" + "=" * 80 + "\n\n")
    
    print("=" * 80)
    print(f"âœ… æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {args.output_file}")
    print("=" * 80)


if __name__ == "__main__":
    main()

